{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZCdQ_Op2S7l"
   },
   "source": [
    "#Homework 1\n",
    "STATS M231A\n",
    "\n",
    "John Baierl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzPjIaK72iU2"
   },
   "source": [
    "***Problem 1:***\n",
    "\n",
    "(1)\n",
    "\n",
    "Consider a multivariate function, $F: \\mathbb{R}^n → \\mathbb{R}$.  The second-order Taylor expanson around $x_0$ is given by:\n",
    "\n",
    "$\n",
    "y = F(x) ≈ F(x_0) + ⟨F'(x_0), x - x_0 ⟩ + \\frac{1}{2}(x - x_0)^T F''(x_0) (x - x_0)\n",
    "$\n",
    "\n",
    "To unpack this, consider a point $x_0 ∈ \\mathbb{R}^n$.  We can reparameterize the function $F$ as follows:\n",
    "\n",
    "$\n",
    "x = x_0 + \\vec{u} t\n",
    "$\n",
    "\n",
    "where $|\\vec{u}| = 1$.  So the 1-D function $f(t)$ will now define the value of $F(x)$ along any direction given by $\\vec{u}$.  This function $f(t)$ is given by:\n",
    "\n",
    "$\n",
    "f(t) = F(x) = F(x + \\vec{u} t)\n",
    "$\n",
    "\n",
    "The 1-D second-order Taylor expansion around $x_0$ is:\n",
    "\n",
    "$\n",
    "f(t) ≈ f(0) + f'(0) t + \\frac{1}{2} f''(0) t^2\n",
    "$\n",
    "\n",
    "From the chain rule:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "f'(t) &= \\frac{\\partial y}{\\partial t} \\\\\n",
    "&= \\frac{\\partial y}{\\partial x^T} \\frac{\\partial x}{\\partial t} \\\\\n",
    "&= F'(x)^T u \\\\\n",
    "&= \\langle F'(x), u \\rangle \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Denote the gradient of $F$ at $x_0$ as $g = F'(x_0)$.  By the definition of the inner product, this becomes:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "f'(0) &= \\langle g, u \\rangle \\\\\n",
    "&= |g| |u| \\cos \\theta \\\\\n",
    "&= |g| \\cos \\theta \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "So as we vary the direction of $\\vec{u}$, the value of $f'(0)$ is maximized when $\\vec{u}$ points in the direction of $g$.  Thus, the gradient describes the direction of maximum increase of $F$.\n",
    "\n",
    "Consider now the second derivative of $f$:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "f''(t) = \\frac{\\partial f'}{\\partial t} &= \\frac{\\partial }{\\partial t} u^T F' \\\\\n",
    "&= u^T \\frac{\\partial}{\\partial t} F' \\\\\n",
    "&= u^T \\frac{\\partial F'}{\\partial x^T} \\frac{\\partial x}{\\partial t} \\\\\n",
    "&= u^T \\frac{\\partial^2 F}{\\partial x \\partial x^T} u\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "We can then define the Hessian as $H = F''(x)$.  So:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "f(t) &\\approx F(x_0) + ⟨F'(x_0), x - x_0 ⟩ + \\frac{1}{2}(x - x_0)^T F''(x_0) (x - x_0) \\\\\n",
    "&= F(x_0) + \\langle g, u \\rangle + \\frac{1}{2} u^T H u t^2.\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "The geometric meaning of the Hessian emerges from its eigen-decomposition $H = Q \\Lambda Q^T$.\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "u^T H u = u^T Q \\Lambda Q^T u\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Defining $v = Q^T u$ turns this into:\n",
    "\n",
    "$\n",
    "u^T H u = v^T \\Lambda v = \\sum_{i = 1}^{n} \\lambda_i v_i^2\n",
    "$\n",
    "\n",
    "This highlights that the Hessian can be viewed as a change of basis to $q_i,\\ldots, q_n$ where the eigenvalues, $\\lambda_i, \\ldots, \\lambda_n$ describe the curvature along each direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGD60YeMflMS"
   },
   "source": [
    "(2)\n",
    "\n",
    "The Newton-Raphson algorithm is an iterative procedure for minimizing (or maximizing) a function by instead optimizing the local quadratic approximation of the function at each step.\n",
    "\n",
    "Consider a multivariate objective function $F: \\mathbb{R} \\rightarrow \\mathbb{R}^n$.  The current iterate is denoted by $x_t$.  From (1), the second-order Taylor expansion of $F$ around $x_t$ is:\n",
    "\n",
    "$\n",
    "F(x) ≈ F(x_0) + ⟨F'(x_0), x - x_0 ⟩ + \\frac{1}{2}(x - x_0)^T F''(x_0) (x - x_0)\n",
    "$\n",
    "\n",
    "For notational simplicity, let $F'(x) = g \\in \\mathbb{R}^n$, and $F''(x) = H \\in \\mathbb{R}^{n \\times n}$.  Differentiating with respect to $x$ yields:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{\\partial F(x)}{\\partial x^T} &= g + \\frac{1}{2} \\frac{\\partial}{\\partial x_T} \\left[ x^T H x - x_0^T H x - x^T H x_0 \\right] \\\\\n",
    "&= g + \\frac{1}{2} (H + H^T) x - H^T x_0 \\\\\n",
    "&= g + H x - H x_0\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Minimizing this quadratic approximation over $x$ will yield the next step, $x_{t + 1}$:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\vec{0} &= g + H x - H x_0 \\\\\n",
    "H x &= H x_0 - g \\\\\n",
    "x &= x_0 - H^{-1} g\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "So for a current iterate, $x_t$, the next step can be found from:\n",
    "\n",
    "$\n",
    "x_{t + 1} = x_t - H^{-1}_t g_t\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZX3fAZh0abA"
   },
   "source": [
    "***Problem 2:***\n",
    "\n",
    "(1)\n",
    "\n",
    "For the logistic regression problem, we have previously derived the log-likelihood for a single observation during class:\n",
    "\n",
    "$\\ell_i (\\beta) = y_i s_i - \\log(1 + e^{s_i})$\n",
    "\n",
    "where $s_i = x_i^T \\beta$.  The full data log-likelihood ($i = 1, \\ldots, n)$ is then simply:\n",
    "\n",
    "$\\ell (\\beta) = \\sum_{i = 1}^{n} y_i s_i - \\log(1 - e^{s_i})$\n",
    "\n",
    "While minimizing this loss function does not have a closed form solution, we can apply the iterated re-weighted least squares algorithm to minimize this function over $\\beta$.  The idea is that, given a current estimate of beta $\\beta_t$, we can search in its neighborhood to improve its estimate.  We can then iterate this procedure until convergence.\n",
    "\n",
    "Let $\\beta = \\beta_t + \\Delta \\beta$, where $\\beta_t$ is our current iterate.  The score of the $i$th observation becomes:\n",
    "\n",
    "$s_i = x_i^T \\beta = x_i^T (\\beta_t + \\Delta \\beta) = x_i^T \\beta_t + x_i^T \\Delta \\beta$\n",
    "\n",
    "The $x_i^T \\beta_t$ term denotes the score at the current prediction ($\\hat{s_i}$), and the $x_i^T \\Delta \\beta$ term denotes the change in the score from the update ($\\Delta s_i$).  Plugging this into the log likelihood for a single observation produces:\n",
    "\n",
    "$\n",
    "\\ell_i (s_i) = \\ell_i (\\hat{s_i} + \\Delta s_i)\n",
    "$\n",
    "\n",
    "Fixing $\\hat{s_i}$ at its current value a applying a second-order Taylor expansion around the current estimate produces\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\ell_i (s_i) &= \\ell_i (\\hat{s_i}) + \\ell_i' (\\hat{s_i}) \\Delta s_i + \\frac{1}{2} \\ell_i''(\\hat{s_i}) \\Delta s_i^2 \\\\\n",
    "&= \\mathrm{const.} + \\hat{e_i} \\Delta s_i - \\frac{1}{2} \\hat{w_i} \\Delta s_i^2 \\\\\n",
    "&= \\mathrm{const.} - \\frac{1}{2} \\hat{w_i} (\\Delta s_i^2 - 2 \\frac{\\hat{e_i}}{\\hat{w_i}} \\Delta s_i) \\\\\n",
    "&= \\mathrm{const.} - \\frac{1}{2} \\hat{w_i} (\\Delta s_i - \\frac{\\hat{e_i}}{\\hat{w_i}})^2\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Above, we made use of the following derivatives of the log-likelihood:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\ell_i' (s_i) &= y_i - \\frac{e^{s_i}}{1 + e^{s_i}} = y_i - p_i = e_i \\\\\n",
    "\\ell_i'' (s_i) &= -p_i(1 - p_i) = -w_i\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Minimizing the last expression for the log-likelihood is equivalent to a weighted least squares problem, where $\\Delta s_i$ is analogous to $x_i^T \\Delta \\beta$, $\\frac{\\hat{e_i}}{\\hat{w_i}}$ is analogous to $\\hat{y_i}$, and $\\hat{w_i}$ is the weight of each observation.  This allows us to easily solve for the next step:\n",
    "\n",
    "$\n",
    "\\Delta \\beta_t = \\left( \\sum_{i = 1}^{n} \\hat{w_i}x_i x_i^T \\right)^{-1} \\left( \\sum_{i = 1}^{n} \\hat{w_i} x_i \\frac{\\hat{e_i}}{\\hat{w_i}} \\right)\n",
    "$\n",
    "\n",
    "We can then repeat until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLjOtL7m00vU"
   },
   "source": [
    "(2)\n",
    "\n",
    "Iterated re-weighted least squares implemented below with $\\beta = (2, -3)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1674964702487,
     "user": {
      "displayName": "JOHN BAIERL",
      "userId": "17288100078065534444"
     },
     "user_tz": 480
    },
    "id": "1TUYG-we0hWu",
    "outputId": "976b5071-3142-4cf2-e603-52a28ac6c916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "beta:  [ 2 -3]\n",
      "beta shape:  (2,)\n",
      "----\n",
      "beta_hat:  [[ 1.40086033]\n",
      " [-2.91406387]]\n",
      "beta_hat shape:  (2, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-0583c92f1c28>:41: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  delta_beta, _, _, _ = np.linalg.lstsq(x_work, y_work)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Set a default seed to make debugging easier\n",
    "np.random.seed(0)\n",
    "def my_logistic(x, y, epsilon = 1e-6):\n",
    "    # ’’’\n",
    "    # Input:\n",
    "    #     x: An n x p matrix each row represents a single data point of dimension p\n",
    "    #     y: An n x 1 vector of labels: True or False, where each entry corresponds to each row in X\n",
    "    #     epsilon: defaulted to 1e-6\n",
    "    # Output:\n",
    "    #     beta: An n x 1 vector of coefficients optimizing the logistic regression problem\n",
    "    #     losses: an array of losses keepin record of the progression\n",
    "    # ’’’\n",
    "    # initialize a list to keep records of all the losses\n",
    "    losses = []\n",
    "    # row column\n",
    "    r, c = x.shape\n",
    "    # initialize our beta vector to be all ones\n",
    "    beta = np.zeros((c, 1))\n",
    "    # run the while loop until we are within an epsilon error\n",
    "    err = 1\n",
    "    while err > epsilon:\n",
    "        # matrix x times vector beta, to produce a vector of signals\n",
    "        s = np.matmul(x, beta)\n",
    "        # compute probability\n",
    "        pr = 1 / (1 + np.exp(-s))\n",
    "        # from class, we showed that l’’(s_i) == -p_i(1 - p_i) = -w_i\n",
    "        # so we compute the weight based on the probability computed from the signal s\n",
    "        w = pr * (1 - pr)\n",
    "        # from class this is equivalent to e_i / w_i\n",
    "        y_hat = (y - pr) / w\n",
    "        # adjust parameters for partial least-squares solution\n",
    "        sw = np.sqrt(w)\n",
    "#        sw_2d = np.expand_dims(sw, axis = 1); weirdly needed this when debugging, but not in loop?\n",
    "        mw = np.repeat(sw_2d, c, axis = 1)\n",
    "        x_work = mw * x\n",
    "        y_work = sw * y_hat\n",
    "        # solve partial least squares problem\n",
    "        delta_beta, _, _, _ = np.linalg.lstsq(x_work, y_work)\n",
    "        # recompute the error\n",
    "        err = np.sum(np.abs(delta_beta))\n",
    "        # keep record of the error\n",
    "        losses.append(err)\n",
    "        # adjust beta\n",
    "        beta = beta + delta_beta\n",
    "    return beta, np.array(losses)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "n = 100\n",
    "p = 2 # Just to make easy to visualize as a plot\n",
    "X = np.random.normal(0, 1, (n, p))\n",
    "print(\"----\")\n",
    "beta = np.array([2, -3])\n",
    "print(\"beta: \", beta)\n",
    "print(\"beta shape: \", beta.shape)\n",
    "print(\"----\")\n",
    "Y = np.random.uniform(0, 1, (n, 1)) < sigmoid(np.dot(X, beta)).reshape((n, 1))\n",
    "beta_hat, losses = my_logistic(X, Y)\n",
    "print(\"beta_hat: \", beta_hat)\n",
    "print(\"beta_hat shape: \", beta_hat.shape)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN71R9jlZ4/gVotKFIYKlhg",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
