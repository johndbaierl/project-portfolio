{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHwXIq2ZyBt4"
   },
   "source": [
    "#Homework 2\n",
    "\n",
    "STATS M231A\n",
    "\n",
    "John Baierl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljGyZ1dYyaIo"
   },
   "source": [
    "**Problem 1)** \n",
    "\n",
    "(1) *XGBoost: derivation and description*\n",
    "\n",
    "In the logistic regression setting, we are interested in predicting a binary outcome $y_i \\in \\{ 0, 1 \\}$ based on a set of predictors $x_1, \\dots , x_p \\in \\mathbb{R}^n$.  Denote the predictors for a single obervation $i \\in \\{ 1, \\dots, n \\}$ by $x_i^T \\in \\mathbb{R}^p$, and the full data matrix $X \\in \\mathbb{R}^{n \\times p}$.\n",
    "\n",
    "Structurally, the idea of boosting is to develop an ensemble or \"committee\" of classification trees, whose results can be summed to produce an output.  So out score, $s$, at the $K^{\\mathrm{th}}$ step is determined by\n",
    "\n",
    "$\n",
    "s = \\sum_{k = 1}^{K} h_k (x)\n",
    "$\n",
    "\n",
    "where $h_k (x)$ is the decision tree from step $k$.  While this is similar to the Random Forest algorithm in its use of decision trees, XGBoost instead grows these trees sequentially to correct for the errors of the current ensemble at each iteration.\n",
    "\n",
    "So fixing the current commitee consisting at step $t$ of $\\sum_{k = 1}^{t - 1} h_k (x) = \\hat{s}$, we would like to solve for the next decision tree $h_t (x)$, making the total score:\n",
    "\n",
    "$s = \\sum_{k = 1}^{t - 1} h_k (x) + h_t (x) = \\hat{s} + \\Delta s$\n",
    "\n",
    "Our goal is to solve for this $\\Delta s$ at each step accoriding to some loss function.  In the classification scheme, our loss function will depend on our partitioning of the support $R$ into $R_1, \\dots, R_m$ and will give us some measure of \"purity\" within each region.  We make paritions be seeking to maximize the information gain with each cut.\n",
    "\n",
    "Using least-squares loss, we can express this as:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\mathrm{Loss} &= \\sum_{i = 1}^{n} (y_i - s_i)^2 \\\\\n",
    "&= \\sum_{i = 1}^{n} (y_i - (\\hat{s_i} + \\Delta s_i))^2 \\\\\n",
    "&= \\sum_{i = 1}^{n} (y_i - \\hat{s_i} - h_t(x))^2\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "So thinking of $\\hat{e_i}$ as our working response, our goal is to fit a tree to minize loss with respect to $\\hat{e_i}$.\n",
    "\n",
    "We now need to determine the fitted values of those residuals within each region to minimize loss.  As derived previously, the log-likelihood for a single observation is given by:\n",
    "\n",
    "$\n",
    "\\ell(s_i) = \\sum_{i = 1}^{n} y_i s_i - \\mathrm{log}(1 + e^{s_i})\n",
    "$\n",
    "\n",
    "A second-order Taylor expansion, our current \"committee\" of classification trees gives us a log likelihood of:\n",
    "\n",
    "$\n",
    "\\ell (s_i) = \\ell(\\hat{s_i}) + \\ell'(\\hat{s_i}) \\Delta s_i + \\frac{1}{2} \\ell''(\\hat{s_i}) \\Delta s_i^2\n",
    "$\n",
    "\n",
    "Using previously computed derivatives of the log-likelihood ($\\ell'(s_i) = y_i - p_i = e_i$; $\\ell''(s_i) = -p_i(1 - p_i) = -w_i$), this becomes:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\ell(s_i) &= \\mathrm{const} + \\hat{e_i} \\Delta s_i - \\frac{1}{2} \\hat{w_i} \\Delta s_i^2 \\\\\n",
    "&= \\mathrm{const} - \\frac{1}{2} \\hat{w_i} \\left( \\Delta s_i^2 - \\frac{\\hat{e_i}}{\\hat{w_i}} \\right)^2 \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "If we let $\\hat{y_i} = \\frac{\\hat{e_i}}{\\hat{w_i}}$ for brevity, we can then express our lost function as:\n",
    "\n",
    "$\n",
    "\\mathrm{Loss} = \\frac{1}{2} \\sum_{i = 1}^{n} \\hat{w_i}(y_i - h_t(x))^2\n",
    "$\n",
    "\n",
    "which can be approached as a least squares problem.  However, we should regularize to prevent overfitting the traning data.  If we denote the predicted value in the $m^{\\mathrm{th}}$ region by $c_m$, we can penalize both by the number of regions ($\\gamma$), and the \"smoothness\" of the fitted predictions ($\\lambda$).  This gives us a loss function of:\n",
    "\n",
    "$\n",
    "\\mathrm{Loss} = \\frac{1}{2} \\sum_{m = 1}^{M} \\sum_{i: x_i \\in R_m} \\hat{w_i}(y_i - c_m)^2 + \\frac{1}{2} \\lambda \\sum_{i = 1}^{M} c_m^2 + \\gamma M\n",
    "$\n",
    "\n",
    "We minimize this loss function by differentiating with respect to $c_m$, we solve for the fitted value in the $m^\\mathrm{th}$ region:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathrm{Loss}}{\\partial c_m} &= -\\sum_{i: x_i \\in R_m} \\hat{w_i} (\\hat{y_i} - c_m) + \\lambda c_m \\\\\n",
    "0 &= \\\\\n",
    "\\hat{c_m} &= \\frac{\\sum_{i: x_i \\in R_m} \\hat{w_i} \\hat{y_i}}{\\sum_{i: x_i \\in R_m}\\hat{w_i} + \\lambda} \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "So after fitting a tree to predict the working residuals, predicting a value of $\\hat{c_m}$ for $x_i$ in that region will minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLYXEks8XmpY"
   },
   "source": [
    "(2) *Adaboost: derivation and description*\n",
    "\n",
    "The Adaboost algorithm bears many similarities in structure to XGBoost for binary classification.  However, rather than fitting a full tree at each iteration, Adaboost instead learns coefficients to fit a series of \"weak learners\" or stumps to make classifications.\n",
    "\n",
    "Here, our score at the $K^\\mathrm{th}$ step is given by:\n",
    "\n",
    "$s = f(x) = \\sum_{k = 1}^{K} \\beta_k h_k(x)$\n",
    "\n",
    "Our goal is to learn the $\\beta_k$ coefficients to combine the $\\{-1, 1 \\}$'s output by our ensemble of weak learners to predict $y \\in \\{ -1, 1\\}$.  Our final output is then:\n",
    "\n",
    "$\n",
    "y = \\mathrm{sign} (s) = \n",
    "\\begin{cases}\n",
    "1 & , s \\geq 0 \\\\\n",
    "-1 & , s < 0\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "To select a loss function, while 0-1 loss is intuitively appealing, it's discontinuity makes it challenging to work with, leading to a non-convex optimization problem.  We can instead use an exponential loss function of the form $\\mathrm{Loss}(s_i) = e^{-ys_i}$.\n",
    "\n",
    "Our boosting algorithm then computes the score at the $t^{\\mathrm{th}}$ step:\n",
    "\n",
    "$\n",
    "s_i = \\sum_{k = 1}^{t - 1} \\beta_k h_k (x_i) + \\beta_t h_t (x_i)\n",
    "$\n",
    "\n",
    "Our objective is to learn $\\beta_t$ and $h_t(x_i)$.  Again, our current ensemble of weak learners is fixed ($\\hat{s_i}$) with our current step to be learned ($\\Delta s_i$).\n",
    "\n",
    "Then our loss function becomes:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\mathrm{Loss} &= \\sum_{i = 1}^{n} e^{-y_i(\\hat{s_i} + \\Delta s_i)} \\\\\n",
    "&= D_i e^{y_i \\Delta s_i}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where $D_i$ defines a distribution of \"attention\" that the $i^{\\mathrm{th}}$ observation recieves based on the loss from the fixed current committee, with the poorly predicted observations recieving more weight.  We can proceed:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\mathrm{Loss} &= \\sum_{i = 1}^{n} D_i e^{-y_i \\beta_t h_t(x_i)} \\\\\n",
    "&= e^{-\\beta_t} \\sum_{i: y_i = h_t(x_i)} D_i + e^{\\beta_t} \\sum_{i: y_i \\neq h_t(x_i)} D_i\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Note here that $\\sum_{i: y_i = h_t(x_i)} D_i = 1 - \\varepsilon$ and $\\sum_{i: y_i \\neq h_t(x_i)} = \\varepsilon$ where $\\varepsilon$ denotes the error rate.\n",
    "\n",
    "This function is minimized when:\n",
    "\n",
    "$\n",
    "\\hat{\\beta} = \\frac{1}{2} \\mathrm{log} \\frac{1 - \\varepsilon}{\\varepsilon}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vh6SkqxX-oPb"
   },
   "source": [
    "(3) *XGBoost vs. Adaboost vs. IRLS:*\n",
    "\n",
    "XBBoost and Adaboost share a similar overall structure in that they iteratively fit learning trees to the \"mistakes\" made by the existing ensemble of learners.  While Adaboost fits only weak learners (i.e. stumps) for classification, XGBoost fits larger decision trees of varying depth.\n",
    "\n",
    "A key difference in these algorithms is that the outputs of the decision trees in XGBoost are continuous values, while the outputs of Adaboost weak learners are binary in $\\{ -1, 1 \\}$.  This difference leads to different choices of loss functions: exponential loss for Adaboost and least square loss for our implementation of XGBoost.  This use of least squares loss allows us to employ the iterated re-weighted least-squares algorithm to learn the decision trees in XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjE6tSmYybIZ"
   },
   "source": [
    "**Problem 2)** Hand-rolled XGBoost for simulated data\n",
    "\n",
    "Code to implement fit an XGBoost model (with `maxdepth = 2`) to the described traning data is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "0lZdwukU6op-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Make simulated data set:\n",
    "n = 100\n",
    "p = 2\n",
    "np.random.seed(42)\n",
    "x1 = np.random.uniform(low = 0, high = 1, size = n)\n",
    "x2 = np.random.uniform(low = 0, high = 1, size = n)\n",
    "X = np.c_[x1, x2]\n",
    "Y = np.square(x1) + np.square(x2) <= np.ones(n)\n",
    "\n",
    "\n",
    "def invlogit(x):\n",
    "  return(1 / (1 + np.exp(-x)))\n",
    "\n",
    "def make_treebranch(x, pred, resid, lam = 0, mingain = 0):\n",
    "  n = x.shape[0]\n",
    "  p = x.shape[1]\n",
    "  sim = np.empty(shape = (n - 1, p))\n",
    "  gains = np.empty(shape = (n, p)) #don't use this for simple example\n",
    "  cuts = np.delete(np.sort(x, axis = 0), n - 1, 0) #removes LARGEST x1 and x2 to avoid div 0 error in loop\n",
    "  for col in np.arange(0, p):\n",
    "    for i in np.arange(0, n - 1):\n",
    "      l = x[:, col] <= cuts[i, col]\n",
    "      r = x[:, col] > cuts[i, col]\n",
    "      l_res = resid[l]\n",
    "      r_res = resid[r]\n",
    "      l_pred = pred[l]\n",
    "      r_pred = pred[r]\n",
    "      l_sim = sum(l_res)**2 / (sum(l_pred * (1 - l_pred)) + lam)\n",
    "      r_sim = sum(r_res)**2 / (sum(r_pred * (1 - r_pred)) + lam)\n",
    "      sim[i, col] = l_sim + r_sim\n",
    "  indx = np.where(sim == sim.max())\n",
    "  rootsim = sum(resid)**2 / (sum(pred * (1 - pred)) + lam)\n",
    "  gain = sim[indx] - rootsim #don't need this for now\n",
    "  cut_best = cuts[indx]\n",
    "  l = np.reshape(x[:, indx[1]], (n, )) <= cut_best\n",
    "  r = np.reshape(x[:, indx[1]], (n, )) > cut_best\n",
    "  dat_l = x[np.reshape(x[:, indx[1]], (n, )) <= cut_best, :]\n",
    "  dat_r = x[np.reshape(x[:, indx[1]], (n, )) > cut_best, :]\n",
    "  res_l = resid[l]\n",
    "  res_r = resid[r]\n",
    "  pred_l = sum(res_l) / (sum(pred[l] * (1 - pred[l])) + lam)\n",
    "  pred_r = sum(res_r) / (sum(pred[r] * (1 - pred[r])) + lam)\n",
    "  pred = l * pred_l + r * pred_r\n",
    "  return(cut_best, indx, gain, pred, pred_l, pred_r)     \n",
    "\n",
    "def my_xgboost(x, y, \n",
    "               eta = 0.3, mingain = 1, lam = 0, cover = 0, \n",
    "               init = 0.5, epsilon = 1e-6):\n",
    "  n = x.shape[0]\n",
    "  p = x.shape[1]\n",
    "  pred = init * np.ones(n) #Initial prediction = 0.5 by default\n",
    "  cutlog = np.empty(shape = (1, 2))\n",
    "  residlog = np.empty(shape = (1, 2))\n",
    "  logodds = np.log(pred / (1 - pred)) #initialize with prediction at step 0 (which is 0, conveniently)\n",
    "  gain = 100 #Arbitrary initialization\n",
    "  i = 0\n",
    "  niter = 0\n",
    "  while gain > mingain:\n",
    "    resid = y - pred\n",
    "    #Fit tree to residuals from previous iteration\n",
    "    tree = make_treebranch(x, pred, resid)\n",
    "    gain = tree[2]\n",
    "    logodds += tree[3]\n",
    "    pred = invlogit(logodds)\n",
    "    cutlog[i] = np.array([tree[0][0], tree[1][1][0]])\n",
    "    cutlog = np.vstack((cutlog, np.empty(shape = (1, 2))))\n",
    "    residlog[i] = np.array((tree[4], tree[5]))\n",
    "    residlog = np.vstack((residlog, np.empty(shape = (1, 2))))\n",
    "    i += 1\n",
    "    niter += 1\n",
    "  return(pred, niter, cutlog, residlog)\n",
    "\n",
    "cutlog = my_xgboost(X, Y)[2]\n",
    "residlog = my_xgboost(X, Y)[3]\n",
    "\n",
    "def my_xgboost_test(x, cutlog, residlog, init = 0.5):\n",
    "  n = x.shape[0]\n",
    "  pred = np.empty(n)\n",
    "  init = np.log(init) / (1 - np.log(init))\n",
    "  for i in np.arange(n):\n",
    "    for j in np.arange(niter):\n",
    "      if x[i, int(cutlog[j, 1])] <= cutlog[j, 0]: #define in xgb function\n",
    "        pred[i] += residlog[j, 0]\n",
    "      else:\n",
    "        pred[i] += residlog[j, 1]\n",
    "  pred = invlogit(pred)\n",
    "  return(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXdRNb8K9s1u"
   },
   "source": [
    "Predicted probabilities for the traning data are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seLi4jOU5uKV",
    "outputId": "e1a32ff9-a4bf-4c85-96e9-9b993c848f8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99967514, 0.00965688, 0.99236874, 0.97927249, 0.94009426,\n",
       "       0.99998609, 0.99975281, 0.00965688, 0.99881037, 0.99881037,\n",
       "       0.99998609, 0.85280975, 0.02760853, 0.99180085, 0.99783307,\n",
       "       0.99180085, 0.99180085, 0.99881037, 0.83815795, 0.99783307,\n",
       "       0.58558174, 0.99180085, 0.99998609, 0.99967514, 0.99881037,\n",
       "       0.87977155, 0.99180085, 0.58558174, 0.99881037, 0.99975281,\n",
       "       0.97927249, 0.99998609, 0.99998609, 0.24586988, 0.00126342,\n",
       "       0.99236874, 0.99975281, 0.99180085, 0.97927249, 0.15491581,\n",
       "       0.94009426, 0.99881037, 0.99975281, 0.85280975, 0.99998609,\n",
       "       0.99881037, 0.99783307, 0.97927249, 0.99881037, 0.99998609,\n",
       "       0.00126342, 0.99236874, 0.85280975, 0.24586988, 0.15491581,\n",
       "       0.85280975, 0.99180085, 0.99180085, 0.99998609, 0.99180085,\n",
       "       0.99425808, 0.99783307, 0.17955735, 0.99783307, 0.99998609,\n",
       "       0.58558174, 0.99998609, 0.99236874, 0.99998609, 0.03579142,\n",
       "       0.17955735, 0.99998609, 0.99975281, 0.99236874, 0.58558174,\n",
       "       0.99236874, 0.17955735, 0.99975281, 0.40186538, 0.99998609,\n",
       "       0.87977155, 0.99881037, 0.94009426, 0.99180085, 0.99998609,\n",
       "       0.99180085, 0.17955735, 0.84323673, 0.03579142, 0.99881037,\n",
       "       0.99998609, 0.02760853, 0.02760853, 0.84323673, 0.87977155,\n",
       "       0.97927249, 0.58558174, 0.83815795, 0.99180085, 0.99180085])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_xgboost(X, Y, mingain = 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hcxDRaO90bz"
   },
   "source": [
    "Next, we repeat the data-generating process and generate test predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "K216GgGByBfs"
   },
   "outputs": [],
   "source": [
    "#Fit model to training data:\n",
    "\n",
    "xg_fit = my_xgboost(X, Y)\n",
    "niter = xg_fit[1]\n",
    "cutlog = xg_fit[2]\n",
    "residlog = xg_fit[3]\n",
    "\n",
    "\n",
    "#Repeat DGP:\n",
    "\n",
    "n = 100\n",
    "p = 2\n",
    "np.random.seed(42)\n",
    "x1_test = np.random.uniform(low = 0, high = 1, size = n)\n",
    "x2_test = np.random.uniform(low = 0, high = 1, size = n)\n",
    "X_test = np.c_[x1_test, x2_test]\n",
    "Y_test = np.square(x1) + np.square(x2) <= np.ones(n)\n",
    "\n",
    "\n",
    "#Test model:\n",
    "\n",
    "testpred = my_xgboost_test(X_test, cutlog, residlog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwxoO8-xtpO2"
   },
   "source": [
    "####***Assessing model fit on test data:***\n",
    "\n",
    "AUC for this model on the test data is determined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "U1HPKhocttj6"
   },
   "outputs": [],
   "source": [
    "#Compute false positive and true positive rates over a grid of thresholds\n",
    "fpr = np.empty(101)\n",
    "tpr = np.empty(101)\n",
    "\n",
    "tot_p = np.sum(Y == True)\n",
    "tot_n = np.sum(Y == False)\n",
    "\n",
    "testpred = my_xgboost_test(X_test, cutlog, residlog)\n",
    "for i in np.arange(start = 0, stop = 101):\n",
    "  output = (testpred >= i / 100)\n",
    "  fpr[i] = sum(output[Y == False]) / tot_n #issue\n",
    "  tpr[i] = sum(output[Y == True]) / tot_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "jFXyzbYsEjtB",
    "outputId": "35b708f8-4a88-46e0-fca5-82a8a65cc5ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'True Positive Rate')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr, tpr)\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odDkBnZaIkua",
    "outputId": "397d92b1-56c4-4137-e2c4-2eae78f434c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VTYYx_ULjFU"
   },
   "source": [
    "We can see that the XGBoost function fits the test data essentially perfectly.  In practice, cross validation should be preformed on the testing data to tune the $\\lambda$ and $\\gamma$ (`mingain`) regularization parameters to optimize testing performance.  However, since the ground truth is a fully determisitic based on $(x_1, x_2)$, we should nonetheless expect an extremely high level of preformace on the test data, since this model is simply learning a rule based on whether a paricular point in $[0, 1]^2$ lies inside or outside of the unit circle.  In real-world applications, we should expect such hyperparameter tuning to improve testing performance, but not achieve this high of an AUC."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
